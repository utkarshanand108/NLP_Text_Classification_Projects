{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Part B — News Article Classification (NLP)\n",
        "**Goal:** Automatically classify news articles into predefined categories (e.g., Politics, Sports, Technology, etc.) using classical NLP: text cleaning → TF-IDF features → linear classifiers → evaluation.\n",
        "\n",
        "**Dataset:** Labeled news entries with `category`, `headline`, `links`, `short_description` (renamed to `text`), and `keywords`.\n",
        "\n",
        "**Plan:**\n",
        "1. Data understanding & cleaning (lowercase, punctuation removal, tokenization, stopwords).  \n",
        "2. Feature extraction with TF-IDF (max 5k features).  \n",
        "3. Baseline models: Logistic Regression, Linear SVM, Multinomial Naive Bayes.  \n",
        "4. Comparison + 5-fold CV (macro-F1).  \n",
        "5. Final evaluation on held-out test set; discuss confusions and next steps.\n",
        "\n",
        "**Deliverables alignment:**  \n",
        "- Clear EDA and preprocessing ✔️  \n",
        "- TF-IDF features and model training ✔️  \n",
        "- Metrics, confusion matrix, and interpretation ✔️  \n",
        "- Story-style Markdown after each code block ✔️  "
      ],
      "metadata": {
        "id": "ww1S8u8k-dKu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Data Collection and Preprocessing"
      ],
      "metadata": {
        "id": "3hrvQ2tOu0qM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2QI2fya2kUEb",
        "outputId": "5dacc5c3-acd2-4fd1-b119-983cfa8c4fe7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 50000 entries, 0 to 49999\n",
            "Data columns (total 5 columns):\n",
            " #   Column             Non-Null Count  Dtype \n",
            "---  ------             --------------  ----- \n",
            " 0   category           50000 non-null  object\n",
            " 1   headline           50000 non-null  object\n",
            " 2   links              50000 non-null  object\n",
            " 3   short_description  49994 non-null  object\n",
            " 4   keywords           47294 non-null  object\n",
            "dtypes: object(5)\n",
            "memory usage: 1.9+ MB\n",
            "None\n",
            "\n",
            "First 5 Rows:\n",
            "   category                                           headline  \\\n",
            "0  WELLNESS              143 Miles in 35 Days: Lessons Learned   \n",
            "1  WELLNESS       Talking to Yourself: Crazy or Crazy Helpful?   \n",
            "2  WELLNESS  Crenezumab: Trial Will Gauge Whether Alzheimer...   \n",
            "3  WELLNESS                     Oh, What a Difference She Made   \n",
            "4  WELLNESS                                   Green Superfoods   \n",
            "\n",
            "                                               links  \\\n",
            "0  https://www.huffingtonpost.com/entry/running-l...   \n",
            "1  https://www.huffingtonpost.com/entry/talking-t...   \n",
            "2  https://www.huffingtonpost.com/entry/crenezuma...   \n",
            "3  https://www.huffingtonpost.com/entry/meaningfu...   \n",
            "4  https://www.huffingtonpost.com/entry/green-sup...   \n",
            "\n",
            "                                   short_description  \\\n",
            "0  Resting is part of training. I've confirmed wh...   \n",
            "1  Think of talking to yourself as a tool to coac...   \n",
            "2  The clock is ticking for the United States to ...   \n",
            "3  If you want to be busy, keep trying to be perf...   \n",
            "4  First, the bad news: Soda bread, corned beef a...   \n",
            "\n",
            "                             keywords  \n",
            "0                     running-lessons  \n",
            "1           talking-to-yourself-crazy  \n",
            "2  crenezumab-alzheimers-disease-drug  \n",
            "3                     meaningful-life  \n",
            "4                    green-superfoods  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the Excel file\n",
        "df = pd.read_excel('data_news.xlsx')\n",
        "\n",
        "# Display basic info and first few rows\n",
        "print(\"Dataset Info:\")\n",
        "print(df.info())\n",
        "\n",
        "print(\"\\nFirst 5 Rows:\")\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Data Collection & Snapshot — What, Why, and First Checks\n",
        "**What this cell does (1–2 lines):**  \n",
        "Loads the news dataset from Excel into a DataFrame and shows its structure and first rows.\n",
        "\n",
        "**Why this matters:**  \n",
        "Before any modeling, we confirm the dataset’s size, columns, and data types to spot obvious issues early (e.g., missing values or wrong dtypes).\n",
        "\n",
        "**What to look for in the output (from _your_ run):**  \n",
        "- **Rows/Columns:** ~50,000 rows and 5 columns were detected in my run.  \n",
        "- **Columns:** `category`, `headline`, `links`, `short_description`, `keywords`.  \n",
        "- **Dtypes:** All shown as `object`.  \n",
        "- **Non-null counts:** Notice `short_description` had a few missing values in later steps and `keywords` had many missing entries; we will handle this downstream.  \n",
        "These observations match the printed `info()` and the first 5 rows preview. ✔️\n",
        "\n",
        "**Interpretation:**  \n",
        "- The dataset is large enough for supervised learning (multi-class text classification).  \n",
        "- Text lives mainly in `short_description`; we’ll rename it to `text` and clean it for modeling.  \n",
        "- We’ll ignore `links` for modeling since it doesn’t contribute to textual semantics directly.  "
      ],
      "metadata": {
        "id": "dk9-QxnC9Uq4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZiLVwKyrmim",
        "outputId": "7d74c049-354b-41c7-db30-ef09e3207520"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text Cleaning — From Raw to Model-Ready\n",
        "**What this cell does (1–2 lines):**  \n",
        "Renames `short_description` to `text`, removes missing entries for `text` and `category`, and creates `clean_text` by lowercasing, stripping punctuation, tokenizing, and removing English stopwords.\n",
        "\n",
        "**Why this matters:**  \n",
        "Machine-learning models need normalized inputs. Cleaning reduces noise (punctuation, case, filler words) so vectorizers (TF-IDF) capture meaningful terms.\n",
        "\n",
        "**What to look for (from _your_ printed example_):**  \n",
        "- **Original sample** vs **Cleaned sample** clearly shows the removal of punctuation and stopwords and the lowercase normalization.  \n",
        "- We keep only informative tokens to strengthen downstream features.\n",
        "\n",
        "**Interpretation:**  \n",
        "The transformation preserves the semantic core while removing noise. This step is essential for stable TF-IDF features and fair model comparison later."
      ],
      "metadata": {
        "id": "uk1kSYZT9l7S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NLTK Resources — Tokenizer & Stopwords\n",
        "**What this cell does (1–2 lines):**  \n",
        "Downloads NLTK resources (`punkt`, `stopwords`) required for tokenizing text and removing common words.\n",
        "\n",
        "**Why this matters:**  \n",
        "Tokenization splits text into words; stopword removal reduces noise (e.g., “the”, “and”), improving the quality of features fed into models.\n",
        "\n",
        "**Interpretation:**  \n",
        "The downloads completed successfully, so the next preprocessing steps that depend on these resources will run without errors."
      ],
      "metadata": {
        "id": "Lp5WKTw49aWJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EFNM_fKYr5Fl",
        "outputId": "554f833e-1094-4187-8f23-115b92a8bb38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3c72af21",
        "outputId": "f6603ea1-0ea8-42e1-aabd-e9b2f02fc0ff"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "df = pd.read_excel('/content/data_news.xlsx')\n",
        "df = df.rename(columns={'short_description': 'text'})\n",
        "df = df.dropna(subset=['text', 'category'])\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    words = word_tokenize(text)\n",
        "    words = [word for word in words if word not in stopwords.words('english')]\n",
        "    return ' '.join(words)\n",
        "\n",
        "df['clean_text'] = df['text'].astype(str).apply(clean_text)\n",
        "\n",
        "print(\"Original Text Example:\\n\", df['text'].iloc[0])\n",
        "print(\"\\nCleaned Text Example:\\n\", df['clean_text'].iloc[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89wkK0BOsD2p",
        "outputId": "059dc538-7d60-452d-a9dd-6841a91621dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text Example:\n",
            " Resting is part of training. I've confirmed what I sort of already knew: I'm not built for running streaks. I'm built for hard workouts three to five days a week with lots of cross training, physical therapy and foam rolling. But I've also confirmed that I'm stubborn with myself.\n",
            "\n",
            "Cleaned Text Example:\n",
            " resting part training ive confirmed sort already knew im built running streaks im built hard workouts three five days week lots cross training physical therapy foam rolling ive also confirmed im stubborn\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing values in all columns\n",
        "print(\"Missing Values per Column:\\n\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Check class distribution\n",
        "print(\"\\nNumber of articles per category:\\n\")\n",
        "print(df['category'].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IN1NNjCytNy5",
        "outputId": "48c9430e-05a0-4dd5-d4d7-614c452d3188"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing Values per Column:\n",
            "\n",
            "category         0\n",
            "headline         0\n",
            "links            0\n",
            "text             0\n",
            "keywords      2706\n",
            "clean_text       0\n",
            "dtype: int64\n",
            "\n",
            "Number of articles per category:\n",
            "\n",
            "category\n",
            "POLITICS          5000\n",
            "ENTERTAINMENT     5000\n",
            "BUSINESS          5000\n",
            "PARENTING         5000\n",
            "WORLD NEWS        5000\n",
            "FOOD & DRINK      5000\n",
            "SPORTS            5000\n",
            "WELLNESS          4999\n",
            "STYLE & BEAUTY    4999\n",
            "TRAVEL            4996\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text Cleaning — From Raw to Model-Ready\n",
        "**What this cell does (1–2 lines):**  \n",
        "Renames `short_description` to `text`, removes missing entries for `text` and `category`, and creates `clean_text` by lowercasing, stripping punctuation, tokenizing, and removing English stopwords.\n",
        "\n",
        "**Why this matters:**  \n",
        "Machine-learning models need normalized inputs. Cleaning reduces noise (punctuation, case, filler words) so vectorizers (TF-IDF) capture meaningful terms.\n",
        "\n",
        "**What to look for (from _your_ printed example_):**  \n",
        "- **Original sample** vs **Cleaned sample** clearly shows the removal of punctuation and stopwords and the lowercase normalization.  \n",
        "- We keep only informative tokens to strengthen downstream features.\n",
        "\n",
        "**Interpretation:**  \n",
        "The transformation preserves the semantic core while removing noise. This step is essential for stable TF-IDF features and fair model comparison later."
      ],
      "metadata": {
        "id": "8JE0lGcm9qoG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Feature Extraction (TF-IDF)\n",
        "\n"
      ],
      "metadata": {
        "id": "F3I2RfhsuPCI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Feature Extraction (TF-IDF)\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X = vectorizer.fit_transform(df['clean_text'])\n",
        "\n",
        "print(\"TF-IDF Matrix Shape:\", X.shape)\n",
        "print(\"Example TF-IDF vector (first row):\")\n",
        "print(X[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U48rJsGbt6f0",
        "outputId": "07c0846b-3334-473f-abe3-0c15e07ff59f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF Matrix Shape: (49994, 5000)\n",
            "Example TF-IDF vector (first row):\n",
            "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
            "\twith 21 stored elements and shape (1, 5000)>\n",
            "  Coords\tValues\n",
            "  (0, 3161)\t0.13101481182350774\n",
            "  (0, 4587)\t0.3409168793545913\n",
            "  (0, 2353)\t0.273745124320764\n",
            "  (0, 914)\t0.364369618648322\n",
            "  (0, 4126)\t0.1799438713463218\n",
            "  (0, 156)\t0.14162813414180941\n",
            "  (0, 2470)\t0.1581205742616256\n",
            "  (0, 2199)\t0.383641652318966\n",
            "  (0, 581)\t0.36304609515876723\n",
            "  (0, 3817)\t0.16157685927859702\n",
            "  (0, 2029)\t0.13661697466699915\n",
            "  (0, 4498)\t0.12987290236117233\n",
            "  (0, 1744)\t0.14034862023841452\n",
            "  (0, 1124)\t0.1279870290912323\n",
            "  (0, 4854)\t0.11713833023116212\n",
            "  (0, 2640)\t0.17600330358409333\n",
            "  (0, 1060)\t0.1982644877678214\n",
            "  (0, 3258)\t0.17261637336779165\n",
            "  (0, 4473)\t0.20102781726680755\n",
            "  (0, 3785)\t0.19463475286548118\n",
            "  (0, 157)\t0.11256134618127646\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature Extraction — TF-IDF (Bag-of-Words with Importance)\n",
        "**What this cell does (1–2 lines):**  \n",
        "Converts `clean_text` into a TF-IDF matrix with up to 5,000 features (most informative terms).\n",
        "\n",
        "**Why this matters:**  \n",
        "TF-IDF highlights words that are frequent in a document but not too common across all documents, giving more discriminative power.\n",
        "\n",
        "**What to look for in the output:**  \n",
        "- **Shape:** Rows ≈ number of samples after drops; Columns = 5,000 features.  \n",
        "- In my run, the matrix was **(49,994 × 5,000)**, confirming ~50K cleaned samples and a fixed feature size.\n",
        "\n",
        "**Interpretation:**  \n",
        "We now have a high-dimensional, sparse numeric representation that classifiers (LogReg, LinearSVC, Naive Bayes) can train on efficiently."
      ],
      "metadata": {
        "id": "gIA-h6Yv99wT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Model Development and Training"
      ],
      "metadata": {
        "id": "M7MzVI-cuY0K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Model Development and Training\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(df['category'])\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "print(\"Model training complete.\")\n",
        "print(\"Sample prediction on first test entry:\", le.inverse_transform([model.predict(X_test[0])[0]]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60t-FuaPuZe3",
        "outputId": "fe9bd6f5-c285-452b-f521-76c70d916ed6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model training complete.\n",
            "Sample prediction on first test entry: ['ENTERTAINMENT']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### First Model — Logistic Regression Baseline\n",
        "**What this cell does (1–2 lines):**  \n",
        "Splits data into train/test (80/20), encodes labels, trains a Logistic Regression classifier on TF-IDF, and shows a sample prediction.\n",
        "\n",
        "**Why this matters:**  \n",
        "A strong linear baseline sets expectations and lets us compare later models fairly on the same split and features.\n",
        "\n",
        "**What to look for:**  \n",
        "- **Sample prediction:** Confirms the pipeline works end-to-end (vectorize → encode → train → predict).  \n",
        "- **max_iter=1000:** Ensures convergence for high-dimensional TF-IDF.\n",
        "\n",
        "**Interpretation:**  \n",
        "This baseline is typically competitive for text classification; we’ll still compare it to Linear SVM and Multinomial Naive Bayes to confirm the best fit."
      ],
      "metadata": {
        "id": "ygKP6HUs-Nvv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 Model Comparison (News)\n",
        "We compare Logistic Regression, Linear SVM, and Multinomial Naive Bayes on the same TF-IDF features and test split."
      ],
      "metadata": {
        "id": "R0zoYncXtQP7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# News — Model Comparison on the existing train/test split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Reuse your existing train/test (X_train, X_test, y_train, y_test)\n",
        "# and your LabelEncoder 'le'\n",
        "\n",
        "models = {\n",
        "    \"LogReg\": LogisticRegression(max_iter=1000),\n",
        "    \"LinearSVC\": LinearSVC(),\n",
        "    \"MultinomialNB\": MultinomialNB()\n",
        "}\n",
        "\n",
        "results = {}\n",
        "for name, clf in models.items():\n",
        "    clf.fit(X_train, y_train)\n",
        "    preds = clf.predict(X_test)\n",
        "    acc = accuracy_score(y_test, preds)\n",
        "    print(f\"\\n=== {name} ===\")\n",
        "    print(\"Accuracy:\", f\"{acc:.4f}\")\n",
        "    # Show class names in the report\n",
        "    print(classification_report(\n",
        "        y_test, preds, target_names=list(le.classes_), zero_division=0, digits=4\n",
        "    ))\n",
        "    results[name] = (acc, clf)\n",
        "\n",
        "best_name = max(results, key=lambda k: results[k][0])\n",
        "best_acc, best_model = results[best_name]\n",
        "print(f\"\\nBest model on test set: {best_name} (Accuracy: {best_acc:.4f})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_IcFi6pYtSCA",
        "outputId": "0af9cc2c-f36a-4d8d-e3e9-bcaefe43c49f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== LogReg ===\n",
            "Accuracy: 0.6431\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "      BUSINESS     0.6171    0.6621    0.6388       947\n",
            " ENTERTAINMENT     0.5269    0.5638    0.5447       972\n",
            "  FOOD & DRINK     0.6781    0.7048    0.6912      1013\n",
            "     PARENTING     0.6578    0.6202    0.6385       998\n",
            "      POLITICS     0.6445    0.5552    0.5966      1032\n",
            "        SPORTS     0.6750    0.6978    0.6862      1006\n",
            "STYLE & BEAUTY     0.7066    0.6647    0.6850       993\n",
            "        TRAVEL     0.6902    0.6465    0.6677      1027\n",
            "      WELLNESS     0.5817    0.6435    0.6110      1007\n",
            "    WORLD NEWS     0.6683    0.6723    0.6703      1004\n",
            "\n",
            "      accuracy                         0.6431      9999\n",
            "     macro avg     0.6446    0.6431    0.6430      9999\n",
            "  weighted avg     0.6452    0.6431    0.6433      9999\n",
            "\n",
            "\n",
            "=== LinearSVC ===\n",
            "Accuracy: 0.6375\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "      BUSINESS     0.6321    0.6822    0.6562       947\n",
            " ENTERTAINMENT     0.5191    0.5319    0.5254       972\n",
            "  FOOD & DRINK     0.6683    0.6900    0.6790      1013\n",
            "     PARENTING     0.6379    0.6232    0.6305       998\n",
            "      POLITICS     0.6264    0.5378    0.5787      1032\n",
            "        SPORTS     0.6773    0.7386    0.7066      1006\n",
            "STYLE & BEAUTY     0.6839    0.6667    0.6752       993\n",
            "        TRAVEL     0.6750    0.6310    0.6522      1027\n",
            "      WELLNESS     0.5943    0.6226    0.6081      1007\n",
            "    WORLD NEWS     0.6590    0.6524    0.6557      1004\n",
            "\n",
            "      accuracy                         0.6375      9999\n",
            "     macro avg     0.6373    0.6376    0.6368      9999\n",
            "  weighted avg     0.6378    0.6375    0.6369      9999\n",
            "\n",
            "\n",
            "=== MultinomialNB ===\n",
            "Accuracy: 0.6292\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "      BUSINESS     0.5915    0.6315    0.6108       947\n",
            " ENTERTAINMENT     0.5703    0.5216    0.5449       972\n",
            "  FOOD & DRINK     0.6555    0.7177    0.6852      1013\n",
            "     PARENTING     0.5329    0.6242    0.5750       998\n",
            "      POLITICS     0.6703    0.5378    0.5968      1032\n",
            "        SPORTS     0.7294    0.6431    0.6836      1006\n",
            "STYLE & BEAUTY     0.6845    0.6445    0.6639       993\n",
            "        TRAVEL     0.6694    0.6310    0.6496      1027\n",
            "      WELLNESS     0.5550    0.6465    0.5972      1007\n",
            "    WORLD NEWS     0.6748    0.6922    0.6834      1004\n",
            "\n",
            "      accuracy                         0.6292      9999\n",
            "     macro avg     0.6334    0.6290    0.6290      9999\n",
            "  weighted avg     0.6340    0.6292    0.6294      9999\n",
            "\n",
            "\n",
            "Best model on test set: LogReg (Accuracy: 0.6431)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Comparison — Which Classifier Fits Best?\n",
        "**What this cell does (1–2 lines):**  \n",
        "Trains **Logistic Regression**, **Linear SVM**, and **Multinomial Naive Bayes** on the same TF-IDF features and test split; prints metrics; selects the best by accuracy.\n",
        "\n",
        "**Why this matters:**  \n",
        "Comparing strong linear baselines helps identify the simplest model that generalizes best before trying heavier approaches (e.g., deep learning).\n",
        "\n",
        "**Results from my run (test set):**  \n",
        "- **LogReg:** Accuracy ≈ **0.643** (best)  \n",
        "- **LinearSVC:** Accuracy ≈ 0.638  \n",
        "- **MultinomialNB:** Accuracy ≈ 0.629\n",
        "\n",
        "**Interpretation:**  \n",
        "- **Logistic Regression** slightly outperforms the others overall, so we’ll treat it as the reference.  \n",
        "- Differences are modest, which is common for well-engineered linear text features.  \n",
        "- Per-class reports show some categories (e.g., SPORTS, FOOD & DRINK) are easier than others (e.g., ENTERTAINMENT) — likely due to more distinctive vocabulary."
      ],
      "metadata": {
        "id": "ImOV-71y-VBV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 5-Fold Cross-Validation (News)\n",
        "We run 5-fold Stratified CV (macro-F1) on the training set for each model to report mean ± std."
      ],
      "metadata": {
        "id": "y0pxYitLtUyC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# News — 5-fold CV on training set (macro-F1)\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from sklearn.metrics import make_scorer, f1_score\n",
        "\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "scorer = make_scorer(f1_score, average='macro')\n",
        "\n",
        "for name, clf in models.items():\n",
        "    scores = cross_val_score(clf, X_train, y_train, cv=cv, scoring=scorer, n_jobs=-1)\n",
        "    print(f\"{name} macro-F1 (5-fold CV on training): {scores.mean():.3f} ± {scores.std():.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nqgcxH1ZtWjD",
        "outputId": "15307130-39c7-40a2-af69-8c390859ce18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LogReg macro-F1 (5-fold CV on training): 0.641 ± 0.009\n",
            "LinearSVC macro-F1 (5-fold CV on training): 0.633 ± 0.007\n",
            "MultinomialNB macro-F1 (5-fold CV on training): 0.626 ± 0.009\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Robustness Check — 5-Fold Stratified CV (Macro-F1)\n",
        "**What this cell does (1–2 lines):**  \n",
        "Runs 5-fold cross-validation on the **training** set to estimate how each model might generalize to unseen data, using **macro-F1** (treats all classes equally).\n",
        "\n",
        "**Why this matters:**  \n",
        "Macro-F1 avoids dominance by frequent classes and is ideal for multi-class tasks with near-balanced labels.\n",
        "\n",
        "**Results from my run (training CV):**  \n",
        "- **LogReg:** Macro-F1 ≈ **0.641 ± 0.009**  \n",
        "- **LinearSVC:** ≈ 0.633 ± 0.007  \n",
        "- **MultinomialNB:** ≈ 0.626 ± 0.009\n",
        "\n",
        "**Interpretation:**  \n",
        "The CV ranking mirrors the test accuracy: **LogReg** remains best. Tight standard deviations suggest stable performance across folds."
      ],
      "metadata": {
        "id": "eaI31eCV-W32"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Model Evaluation"
      ],
      "metadata": {
        "id": "rISOSVBBufVF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Model Evaluation\n",
        "\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(\"Accuracy Score:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred, target_names=le.classes_))\n",
        "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CfOje8aTuhsq",
        "outputId": "d65aef19-8b88-4cb7-a2b2-a8f214840c10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy Score: 0.643064306430643\n",
            "\n",
            "Classification Report:\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "      BUSINESS       0.62      0.66      0.64       947\n",
            " ENTERTAINMENT       0.53      0.56      0.54       972\n",
            "  FOOD & DRINK       0.68      0.70      0.69      1013\n",
            "     PARENTING       0.66      0.62      0.64       998\n",
            "      POLITICS       0.64      0.56      0.60      1032\n",
            "        SPORTS       0.68      0.70      0.69      1006\n",
            "STYLE & BEAUTY       0.71      0.66      0.69       993\n",
            "        TRAVEL       0.69      0.65      0.67      1027\n",
            "      WELLNESS       0.58      0.64      0.61      1007\n",
            "    WORLD NEWS       0.67      0.67      0.67      1004\n",
            "\n",
            "      accuracy                           0.64      9999\n",
            "     macro avg       0.64      0.64      0.64      9999\n",
            "  weighted avg       0.65      0.64      0.64      9999\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            " [[627  27  20  29  61  27  16  28  67  45]\n",
            " [ 36 548  45  44  46  90  68  25  36  34]\n",
            " [ 21  51 714  32  14  27  35  55  54  10]\n",
            " [ 36  47  44 619  23  26  36  30 124  13]\n",
            " [ 86  54  21  35 573  50  19  25  45 124]\n",
            " [ 18 106  27  16  34 702  16  28  21  38]\n",
            " [ 36  88  44  41  13  30 660  31  41   9]\n",
            " [ 31  54  71  36  20  24  42 664  48  37]\n",
            " [ 58  36  59  63  24  29  27  38 648  25]\n",
            " [ 67  29   8  26  81  35  15  38  30 675]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Final Evaluation — Accuracy, Reports, and Confusions\n",
        "**What this cell does (1–2 lines):**  \n",
        "Evaluates the chosen model on the **held-out test set** and prints overall accuracy, a per-class classification report, and the confusion matrix.\n",
        "\n",
        "**Why this matters:**  \n",
        "This is the **official** performance we would report. The confusion matrix reveals where the model confuses categories and why.\n",
        "\n",
        "**Key results from my run:**  \n",
        "- **Accuracy:** ≈ **0.643** on 10 classes.  \n",
        "- **Per-class precision/recall/F1:** Stronger on categories with distinctive vocab (e.g., SPORTS, FOOD & DRINK) and slightly weaker on more overlapping themes (e.g., ENTERTAINMENT).  \n",
        "- **Notable confusions (examples observed):**  \n",
        "  - **POLITICS ↔ WORLD NEWS:** Frequent cross-predictions (e.g., many POLITICS labeled as WORLD NEWS), suggesting overlapping geopolitical terms.  \n",
        "  - **ENTERTAINMENT ↔ SPORTS/STYLE:** Some headlines share celebrity names or lifestyle terms, causing spillover.\n",
        "\n",
        "**Interpretation and next steps:**  \n",
        "- The ~0.64 accuracy is reasonable for a lightweight linear baseline on short descriptions.  \n",
        "- **Improvements to try:**  \n",
        "  1) Add `headline` text to `text` (concatenate) to give more context.  \n",
        "  2) Tune TF-IDF (e.g., `ngram_range=(1,2)`, `min_df`, `max_df`).  \n",
        "  3) Calibrate class weights or try **LinearSVC** with probability calibration for different metrics.  \n",
        "  4) Try more expressive models (e.g., **Logistic Regression + n-grams**, **SGDClassifier**, or transformer embeddings) if resources allow.  \n",
        "  5) Visualize the confusion matrix as a heatmap and add top-words per class to explain model decisions for the report.\n",
        "\n",
        "**One-line takeaway (for the report):**  \n",
        "A TF-IDF + Logistic Regression baseline achieves ~0.64 accuracy on 10 news categories; errors concentrate on semantically adjacent topics (e.g., POLITICS vs WORLD NEWS), pointing to the value of adding more context and richer features."
      ],
      "metadata": {
        "id": "Ao2AbiOj-anr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Google Drive Link to Codes, reports, presentations and video explanations for the project: https://drive.google.com/drive/folders/1gULvQYdkDMaHAtvwXY2bkE0FrExAVH3D?usp=sharing"
      ],
      "metadata": {
        "id": "FgHCz5OUYwIu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "By, Utkarsh Anand"
      ],
      "metadata": {
        "id": "JXQ4t9dBZUct"
      }
    }
  ]
}